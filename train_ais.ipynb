{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "files = os.listdir('/content/drive/MyDrive/for_colab/.')\n",
    "print(files)\n",
    "\n",
    "# Specify the path of the file in your Google Drive\n",
    "npy_data_path = '/content/drive/MyDrive/for_colab/allObjectsTwitterEncoded.npy'\n",
    "twitter_data_path = '/content/drive/MyDrive/for_colab/Twitter_Data[1].csv'\n",
    "\n",
    "# Read the file using appropriate methods (e.g., pandas, numpy, etc.)\n",
    "# Example for reading a CSV file using pandas:\n",
    "import pandas as pd\n",
    "data = pd.read_csv(twitter_data_path)\n",
    "\n",
    "data.to_csv('twitter_data.csv')\n",
    "np.save('allObjectsTwitterEncoded.npy', np.load(npy_data_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(acc(data=npy_data_path, n_epochs=30, hidden_size=128, lr=0.004, criterion=nn.MSELoss()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell generates AllObjectsTwitterEncoded.npy and .csv files if provided with a Twitter dataset that has at least 2000 tweets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d {Twitter Sentiment Analysis}\n",
    "# !pip install openai==0.28  # TODO: Upgrade your code to most recent version.\n",
    "# from rnn import train_model\n",
    "import json\n",
    "import numpy as np\n",
    "# import openai\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "iterator = 0\n",
    "twitter = pd.read_csv('twitter_data.csv')\n",
    "twitter = twitter['clean_text']\n",
    "twitter = twitter.dropna(axis = 0, how = 'all')\n",
    "twitter = twitter.reset_index(drop=True)\n",
    "print(twitter[147])\n",
    "print(twitter[148])\n",
    "print(twitter[149])\n",
    "\n",
    "\n",
    "PRE_ENC_LENGTH = 1050\n",
    "PRE_RNN_HIDDEN = 2000\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "MODEL = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "\n",
    "\n",
    "# openai.api_key = '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'\n",
    "\n",
    "# These two are commented out because they contain boolean lists that need to be written out.\n",
    "'''\n",
    "    'mot': {\n",
    "        'texts': ['zu_was_beschreibung'],\n",
    "        'bools': ['wie', 'positive_faktoren', 'negative_faktoren'],  # TODO: Expand the factors!\n",
    "        'scalars': [],\n",
    "        'single_ids': [],\n",
    "        'list_ids': ['wer', 'zu_was_fuer_objekten', 'von_wem']\n",
    "    },\n",
    "\n",
    "    'bea': {\n",
    "        'texts': ['aussehen'],\n",
    "        'bools': ['art'],  # TODO: Ausschreiben\n",
    "        'scalars': ['difficulty'],\n",
    "        'single_ids': ['wo'],\n",
    "        'list_ids': []\n",
    "    },\n",
    "'''\n",
    "\n",
    "text_features_to_prompts = {\n",
    "    'name': 'Give me the name of a fictional character',\n",
    "    'backstory': 'Give me the backstory of a fictional character',\n",
    "    'was': 'Give me a short description of what could happen at a fictional scene in a Theatre I am writing',\n",
    "    'warum': 'Give me conditions for a scene in my self-written theatre to occur like who needs to be on stage',\n",
    "}\n",
    "\n",
    "all_features = {\n",
    "    'sci': {\n",
    "        'texts': ['name', 'backstory'],\n",
    "        'bools': ['charakterbogen', 'plaene_fuer_den_charakter', 'hat_eine_backstory'],\n",
    "        'scalars': [],\n",
    "        'single_ids': [],\n",
    "        'list_ids': ['startszene', 'events', 'gruppen', 'backstory_sonstiges']\n",
    "    },\n",
    "    'eus': {\n",
    "        'texts': ['was', 'warum'],\n",
    "        'bools': ['untersuchen', 'soziale_interaktion', 'fight', 'start'],\n",
    "        'scalars': ['schwierigkeitsgrad', 'wahrscheinlichkeit'],\n",
    "        'single_ids': [],\n",
    "        'list_ids': ['wer', 'wo', 'Gegenstände', 'Geheimnisse', 'personen', 'wer_muss_da_sein', 'wo_kann_das_sein',\n",
    "                     'motivationen']\n",
    "    },\n",
    "    'npc': {\n",
    "        'texts': ['name', 'backstory'],\n",
    "        'bools': ['charakterbogen', 'plaene', 'hat_eine_backstory'],\n",
    "        'scalars': [],\n",
    "        'single_ids': [],\n",
    "        'list_ids': ['events_und_szenen', 'gruppen', 'backstory_sonstiges']\n",
    "    },\n",
    "    'geh': {\n",
    "        'texts': ['was'],\n",
    "        'bools': [],\n",
    "        'scalars': ['positivitaet'],\n",
    "        'single_ids': [],\n",
    "        'list_ids': ['wer_weiss_davon', 'wen_und_was_betrifft_das']\n",
    "    },\n",
    "    'gru': {\n",
    "        'texts': ['grund_des_zusammenhalts'],\n",
    "        'bools': [],\n",
    "        'scalars': [],\n",
    "        'single_ids': ['moegliche_motivation_von_aussen', 'geburtsort_der_gruppe'],\n",
    "        'list_ids': []\n",
    "    },\n",
    "    'geg': {\n",
    "        'texts': ['was'],\n",
    "        'bools': [],\n",
    "        'scalars': ['wert'],\n",
    "        'single_ids': [],\n",
    "        'list_ids': ['wessen', 'wo']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "This class is the central structure for an adventure. It's supposed to be convertible to virtually any other\n",
    "possible representation of an adventure. To save this as JSON works already. Currently I am working on a computer\n",
    "readable representation of an adventure (in a high-dimensional vector field). Also I have in mind a full text\n",
    "representation, maybe a representation that uses a lot of graphics, a representation that would work as a computer\n",
    "game like the AI-RPG project, the adventure as a board game and so on.\n",
    "'''\n",
    "class Adventure:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sci = ObjectClass('sci',\n",
    "                               name=str,\n",
    "                               charakterbogen=bool,\n",
    "                               plaene_fuer_den_charakter=bool,\n",
    "                               startszene=(list, str),  # list of events and scenes (where start-scene is true)\n",
    "                               events=(list, str),  # list of events and scenes\n",
    "                               gruppen=(list, str),  # list of groups\n",
    "                               hat_eine_backstory=bool,\n",
    "                               backstory=str,\n",
    "                               backstory_sonstiges=(list, str)\n",
    "                               )\n",
    "        self.mot = ObjectClass('mot',\n",
    "                               wer=(list, str),  # list of Persons (PCs and NPCs) and groups\n",
    "                               zu_was_beschreibung=str,\n",
    "                               zu_was_fuer_objekten=(list, str),\n",
    "                               wie=bool,  # always True\n",
    "                               positive_faktoren=(list, bool),\n",
    "                               negative_faktoren=(list, bool),\n",
    "                               # TODO: beide vollständig ausschreiben. Listen sind reserviert für unklar lange Listen.\n",
    "                               # both factors are exactly 10 bools, each hardcoded to the emotions from the Notizbuch.\n",
    "                               von_wem=(list, str)  # list of Persons ??\n",
    "                               )\n",
    "        self.eus = ObjectClass('eus',\n",
    "                               wer=list,  # this seems wrong!\n",
    "                               wo=(list, str),\n",
    "                               was=str,\n",
    "                               untersuchen=bool,\n",
    "                               Gegenstände=(list, str),  # list of Gegenstände\n",
    "                               Geheimnisse=(list, str),  # list of secrets\n",
    "                               soziale_interaktion=bool,  # is it a scene of social interaction?\n",
    "                               personen=(list, str),  # list of persons whose relation to the players might change\n",
    "                               fight=bool,  # is it a fight scene?\n",
    "                               schwierigkeitsgrad=float,\n",
    "                               warum=str,\n",
    "                               wer_muss_da_sein=(list, str),  # list of persons\n",
    "                               wo_kann_das_sein=(list, str),  # list of locations\n",
    "                               start=bool,\n",
    "                               wahrscheinlichkeit=float,\n",
    "                               motivationen=(list, str)\n",
    "                               )\n",
    "        # TODO: Orte\n",
    "        self.npc = ObjectClass('npc',\n",
    "                               name=str,\n",
    "                               charakterbogen=bool,  # hat einen Charakterbogen?\n",
    "                               plaene=bool,  # es gibt Zukunftspläne für diesen NPC\n",
    "                               events_und_szenen=(list, str),  # list of events\n",
    "                               gruppen=(list, str),  # list of groups\n",
    "                               hat_eine_backstory=bool,\n",
    "                               backstory=str,\n",
    "                               backstory_sonstiges=(list, str)\n",
    "                               )\n",
    "        self.geh = ObjectClass('geh',\n",
    "                               was=str,\n",
    "                               wer_weiss_davon=(list, str),  # list of Personen\n",
    "                               wen_und_was_betrifft_das=(list, str),  # list of persons, Gegenstände und Orten\n",
    "                               positivitaet=float  # how positive is this secret to the players.\n",
    "                               )\n",
    "        self.gru = ObjectClass('gru',\n",
    "                               grund_des_zusammenhalts=str,\n",
    "                               moegliche_motivation_von_aussen=str,  # ??, ids are strings\n",
    "                               geburtsort_der_gruppe=str  # roomID, Geburtsort der Gruppe\n",
    "                               )\n",
    "        self.bea = ObjectClass('bea',\n",
    "                               art=(list, bool),  # TODO Ausschreiben!\n",
    "                               difficulty=float,  # how big of a challenge does this beast pose.\n",
    "                               wo=str,  # roomIDs\n",
    "                               aussehen=str\n",
    "                               )\n",
    "        self.geg = ObjectClass('geg',\n",
    "                               wessen=(list, str),  # list of Persons\n",
    "                               wert=float,\n",
    "                               was=str,\n",
    "                               wo=(list, str)  # list of locations\n",
    "                               )\n",
    "\n",
    "    def save(self, path='adventure.json'):\n",
    "        to_save = {}\n",
    "        for i in [self.sci, self.mot, self.eus, self.npc, self.geh, self.gru, self.bea, self.geg]:\n",
    "            to_save.update(i.to_save())\n",
    "        with open(path, 'w+') as f:\n",
    "            f.write(json.dumps(to_save, indent=4))\n",
    "\n",
    "    def load(self, path='adventure.json'):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for i in [self.sci, self.mot, self.eus, self.npc, self.geh, self.gru, self.bea, self.geg]:\n",
    "            i.all_objects = data[i.name]\n",
    "            i.id_counter = len(data[i.name])\n",
    "\n",
    "    def to_list(self):\n",
    "        to_save = {}\n",
    "        for i in [self.sci, self.mot, self.eus, self.npc, self.geh, self.gru, self.bea, self.geg]:\n",
    "            to_save.update(i.to_save())\n",
    "        return json.dumps(to_save, indent=4)\n",
    "\n",
    "    def to_text(self):\n",
    "        return 'Adventure to text doesn\\'t really work yet.'\n",
    "\n",
    "\n",
    "# This class is more or less an add-on to the adventure class.\n",
    "class ObjectClass:\n",
    "    def __init__(self, class_name, **features):\n",
    "        self.name = class_name\n",
    "        self.features = features\n",
    "        self.id_counter = 0\n",
    "        self.all_objects = []\n",
    "\n",
    "    def add(self, **features_values):\n",
    "        for i, val in features_values.items():\n",
    "            if i not in list(self.features.keys()):\n",
    "                raise ValueError\n",
    "            else:\n",
    "                if isinstance(self.features[i], tuple):\n",
    "                    if not isinstance(val, list):\n",
    "                        raise ValueError\n",
    "                    if not isinstance(val[0], self.features[i][1]):\n",
    "                        raise ValueError\n",
    "                elif not isinstance(val, self.features[i]):\n",
    "                    raise ValueError\n",
    "        object_id = f'id_{self.name[0:3]}_{self.id_counter}'\n",
    "        features_values.update({'ID': object_id})\n",
    "        self.id_counter += 1\n",
    "        self.all_objects.append(features_values)\n",
    "        return object_id\n",
    "\n",
    "    def to_save(self):\n",
    "        return {self.name: self.all_objects}\n",
    "\n",
    "\n",
    "# This is not up-to-date. It generates a demo-adventure about Max Mustermann.\n",
    "def demo_adventure():\n",
    "    adv = Adventure('demo')\n",
    "    # Max once met a monster which he now meets again in the very first scene.\n",
    "    # Max wants revenge and intends to kick the monster with his boots.\n",
    "    # John also exists. He knows that Max once met the monster.\n",
    "    # John and Max are a group.\n",
    "    adv.sci.add(\n",
    "        name='Max',\n",
    "        charakterbogen=False,\n",
    "        plaene_fuer_den_charakter=True,\n",
    "        startszene=['id_Eve_1'],  # list of events and scenes (where start-scene is true)\n",
    "        # events=[],  # list of events and scenes\n",
    "        gruppen=['id_Gru_1'],  # list of groups\n",
    "        hat_eine_backstory=True,\n",
    "        backstory='This is Max awesome backstory. Max was born in Musterhausen. He was once attacked by a monster.',\n",
    "        backstory_sonstiges=['id_Bea_1']\n",
    "    )\n",
    "    adv.mot.add(\n",
    "        wer=['id_Spi_1'],  # list of Persons (PCs and NPCs) and groups\n",
    "        zu_was_beschreibung='Max will sich am Monster rächen indem er es mit seinen Stiefeln tritt.',\n",
    "        zu_was_fuer_objekten=['id_Geg_1'],\n",
    "        wie=True,  # always True\n",
    "        positive_faktoren=[False, False, False, False, False, False, False, False, False, False],\n",
    "        # exactly 10 bools, each hardcoded to the emotions from the notizbuch\n",
    "        negative_faktoren=[True, False, True, False, False, False, False, False, True, False],\n",
    "        # von_wem=(list, str)  # he hasn't been motivated by anyone on the outside.\n",
    "    )\n",
    "    adv.eus.add(\n",
    "        wer=['id_Spi_1', 'id_Bea_1'],\n",
    "        wo=['id_Ort_1_leidergibtesnochkeineorte'],\n",
    "        was='Max meets the monster that once attacked him again.',\n",
    "        untersuchen=False,\n",
    "        Gegenstände=['id_Geg_1'],  # list of Gegenstände\n",
    "        Geheimnisse=['id_Geh_1'],  # list of secrets\n",
    "        soziale_interaktion=False,  # is it a scene of social interaction?\n",
    "        # personen=(list, str),  # since its no social interaction the SC can't change any social relations.\n",
    "        fight=True,  # is it a fight scene?\n",
    "        schwierigkeitsgrad=0.8,\n",
    "        warum='Max und Monster sind am gleichen Ort.?!',\n",
    "        # wer_muss_da_sein=(list, str),  # list of persons  # muss nicht unbedingt was hin.\n",
    "        # wo_kann_das_sein=(list, str),  # list of locations  # dito\n",
    "        start=True,\n",
    "        wahrscheinlichkeit=1.,\n",
    "        motivationen=['id_Mot_1']\n",
    "    )\n",
    "    # TODO: Orte\n",
    "    adv.npc.add(\n",
    "        name='John',\n",
    "        charakterbogen=False,  # hat einen Charakterbogen?\n",
    "        plaene=False,  # es gibt Zukunftspläne für diesen NPC\n",
    "        events_und_szenen=['id_Eve_1'],  # list of events\n",
    "        gruppen=['id_Gru_1'],  # list of groups\n",
    "        hat_eine_backstory=True,\n",
    "        backstory='John is the one who originally sold Max his boots.',\n",
    "        backstory_sonstiges=['id_Spi_1', 'id_Geg_1']\n",
    "    )\n",
    "    adv.geh.add(\n",
    "        was='Max once was attacked by the monster in his childhood.',\n",
    "        wer_weiss_davon=['id_Spi_1', 'id_NPC_1'],  # list of Personen\n",
    "        wen_und_was_betrifft_das=['id_Spi_1', 'id_Bea_1'],  # list of persons, Gegenstände und Orten\n",
    "        positivitaet=0.2  # how positive is this secret to the players.\n",
    "    )\n",
    "    adv.gru.add(\n",
    "        grund_des_zusammenhalts='John and Max are very good friends.',\n",
    "        # moegliche_motivation_von_aussen=str,  # There is no motivation from the outside\n",
    "        # geburtsort_der_gruppe=str  # roomID, Geburtsort der Gruppe\n",
    "    )\n",
    "    adv.bea.add(\n",
    "        # art=(list, bool),\n",
    "        difficulty=0.8,  # how big of a challenge does this beast pose.\n",
    "        wo='id_Ort_1_leidergibtesnochkeineorte',  # roomIDs\n",
    "        aussehen='This beast is a big Monster that seem really quite threatening.'\n",
    "    )\n",
    "    adv.geg.add(\n",
    "        wessen=['id_Spi_1'],  # list of Persons\n",
    "        wert=2.,\n",
    "        was='anti-monster-Boots',\n",
    "        # wo=[]  # Wo Max halt ist.\n",
    "    )\n",
    "    # adv.save('demo_adventure.json')\n",
    "    return adv\n",
    "\n",
    "\n",
    "# returns a high-dimensional (1024) vector representation of the passed in sentence.\n",
    "def roberta(sentence):\n",
    "    # from https://huggingface.co/sentence-transformers/all-roberta-large-v1\n",
    "\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    sentences = [sentence]\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    # I made this global variables because they take years to load so best just do it once.\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = TOKENIZER(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = MODEL(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    # print(\"Sentence embeddings:\")\n",
    "    # print(sentence_embeddings)\n",
    "    return sentence_embeddings.tolist()[0]\n",
    "\n",
    "\n",
    "def rnn_pres(list_of_ids, id_to_pre):\n",
    "    pass  # this function is supposed to return the output of the RNN encoder when fed by the pre_encoding of the\n",
    "    # objects of list_of_ids\n",
    "    return list(range(PRE_RNN_HIDDEN))  # this has the same length\n",
    "\n",
    "print(iterator)\n",
    "\n",
    "# this function takes an object (by id) and returns an encoding which is either a pre_encoding (ignoring ids) or,\n",
    "# if id_to_pre is not None the full encoding.\n",
    "def enc_obj(obj_class, id, id_to_pre=None, handle_id_lists=rnn_pres):\n",
    "    global iterator\n",
    "    print(iterator)\n",
    "    features = all_features[obj_class.name]\n",
    "    for i in ['texts', 'bools', 'scalars', 'single_ids', 'list_ids']:\n",
    "        if i not in features.keys():\n",
    "            features.update({i: []})\n",
    "    f_v = obj_class.all_objects[int(id[7:]) - 1]  # =features_values\n",
    "    enc = []\n",
    "\n",
    "    # deal with actual texts ; 1024 Values all together\n",
    "    text = 'This is text.'\n",
    "    for n in features['texts']:\n",
    "        if n in f_v.keys():\n",
    "            text = f'{text}\\n{n}: {f_v[n]}'\n",
    "    text = twitter.loc[iterator]\n",
    "    iterator += 1\n",
    "    # get some unique text, for example from twitter.\n",
    "    text_embedding = roberta(text)\n",
    "    for i in text_embedding:\n",
    "        enc.append(i)\n",
    "\n",
    "    # deal with booleans; 2 Values each\n",
    "    for n in features['bools']:\n",
    "        if n in f_v.keys():  # 2 values.\n",
    "            enc.append(1.)\n",
    "            if f_v[n]:\n",
    "                enc.append(1.)\n",
    "            else:\n",
    "                enc.append(0.)\n",
    "        else:\n",
    "            enc.append(0.)\n",
    "            enc.append(0.)\n",
    "\n",
    "    # deal with scalars; 2 Values each\n",
    "    for n in features['scalars']:\n",
    "        if n in f_v.keys():\n",
    "            enc.append(1.)\n",
    "            enc.append(float(f_v[n]))\n",
    "        else:\n",
    "            enc.append(0.)\n",
    "            enc.append(0.)\n",
    "\n",
    "    # check length\n",
    "    expected_length = {'sci': 1030, 'eus': 1036, 'npc': 1030, 'geh': 1026, 'gru': 1024, 'bea': 1028,\n",
    "                       'geg': 1026}  # TODO: add mot\n",
    "    if len(enc) != expected_length[obj_class.name]:\n",
    "        raise ValueError\n",
    "    # fill up with zeros then return if done.\n",
    "    for i in range(PRE_ENC_LENGTH - len(enc)):\n",
    "        enc.append(0)\n",
    "    if id_to_pre is None:\n",
    "        return enc\n",
    "\n",
    "    # deal with single ids; PRE_ENC_LENGTH values each\n",
    "    for n in features['single_ids']:\n",
    "        if n in f_v:\n",
    "            enc.append(1.)\n",
    "            for i in id_to_pre[f_v[n]]:\n",
    "                enc.append(i)\n",
    "        else:\n",
    "            enc.append(0.)\n",
    "            for i in range(PRE_ENC_LENGTH):\n",
    "                enc.append(0.)\n",
    "\n",
    "    # deal with list of ids; PRE_RNN_HIDDEN values each (=per list)\n",
    "    for n in features['list_ids']:\n",
    "        if n in f_v:\n",
    "            enc.append(1.)\n",
    "            eve = handle_id_lists(f_v[n], id_to_pre)\n",
    "            for i in eve:\n",
    "                enc.append(i)\n",
    "        else:\n",
    "            enc.append(0.)\n",
    "            for i in range(PRE_RNN_HIDDEN):\n",
    "                enc.append(0.)\n",
    "\n",
    "    return enc\n",
    "\n",
    "\n",
    "# This function writes an adventure with every mathematically possible object.\n",
    "def generate_adventure_objs():\n",
    "    adv = Adventure(name='AllObjects')\n",
    "    all_options = {}\n",
    "\n",
    "    for cla in all_features.keys():\n",
    "        opt = {}\n",
    "        for b in all_features[cla]['bools']:\n",
    "            opt.update({b: [False, True]})\n",
    "\n",
    "        for s in all_features[cla]['scalars']:\n",
    "            opt.update({s: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]})\n",
    "\n",
    "        for t in all_features[cla]['texts']:\n",
    "            opt.update({t: [t]})\n",
    "        all_options.update({cla: opt})\n",
    "\n",
    "    # generate objs:\n",
    "\n",
    "    def iter_(idcs_, maxs_):\n",
    "        if idcs_ == []:\n",
    "            return None, None\n",
    "        if 0 in maxs_:\n",
    "            raise ValueError\n",
    "        idcs_[-1] += 1\n",
    "        x = 0\n",
    "        for i in range(len(idcs_)):\n",
    "            idx = idcs_[-(i + 1)]\n",
    "            max = maxs_[-(i + 1)]\n",
    "            x += 1\n",
    "            if idx == max:\n",
    "                idcs_[-x] = 0\n",
    "                if x == len(idcs_):\n",
    "                    return None, None\n",
    "                idcs_[-(x + 1)] += 1\n",
    "        return idcs_, maxs_\n",
    "\n",
    "    def create_obj(opt, idcs, cla, adv):\n",
    "        # TODO: Debug: Why is this not called or doesn't work?\n",
    "        name_to_feat = {'sci': adv.sci, 'mot': adv.mot, 'eus': adv.eus, 'npc': adv.npc, 'geh': adv.geh, 'gru': adv.gru,\n",
    "                        'bea': adv.bea, 'geg': adv.geg}\n",
    "        parameter = {}\n",
    "        for o, idx in zip(opt.items(), idcs):\n",
    "            if not isinstance(o[1][idx], str):\n",
    "                parameter.update({o[0]: o[1][idx]})\n",
    "            else:\n",
    "                prompt = f'Give me a very short fascinating story consisting of up to five sentences:\\n\\n'\n",
    "                # response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=2.,\n",
    "                #                                    max_tokens=200)\n",
    "                # response = response['choices'][0]['text']\n",
    "                # parameter.update({o[0]: response})\n",
    "                parameter.update({o[0]: prompt})\n",
    "        # TODO: texts!\n",
    "        name_to_feat[cla].add(**parameter)\n",
    "\n",
    "    print('start writing')\n",
    "    for cla in all_features.keys():\n",
    "        print(cla)\n",
    "        opt = all_options[cla]\n",
    "        idcs = [0 for _ in opt.keys()]\n",
    "        maxs = [len(i) for i in opt.values()]\n",
    "        while idcs is not None:\n",
    "            create_obj(opt, idcs, cla, adv)\n",
    "            idcs, maxs = iter_(idcs, maxs)\n",
    "\n",
    "    return adv\n",
    "\n",
    "\n",
    "# This function generates a handful of objects and prints the result of enc_obj for each.\n",
    "def test():\n",
    "    adv = demo_adventure()\n",
    "    # adv = Adventure(name='demo')\n",
    "    # adv.load('demo_adventure.json')\n",
    "    adv.sci.add(\n",
    "        name='Alfred',\n",
    "        charakterbogen=True,\n",
    "        # plaene_fuer_den_charakter=True,\n",
    "        # startszene=['id_Eve_1'],  # list of events and scenes (where start-scene is true)\n",
    "        # events=[],  # list of events and scenes\n",
    "        gruppen=['id_Gru_1'],  # list of groups\n",
    "        hat_eine_backstory=True,\n",
    "        backstory='This is Max awesome backstory. Max was born in Musterhausen. He was once attacked by a monster.',\n",
    "        backstory_sonstiges=['id_sci_1']\n",
    "    )\n",
    "    adv.sci.add(\n",
    "        name='Berta',\n",
    "        charakterbogen=False,\n",
    "        plaene_fuer_den_charakter=True,\n",
    "        startszene=['id_Eve_1'],  # list of events and scenes (where start-scene is true)\n",
    "        events=['id_Eve_1'],  # list of events and scenes\n",
    "        gruppen=['id_Gru_1'],  # list of groups\n",
    "        hat_eine_backstory=True,\n",
    "        backstory='This is Max awesome backstory. Max was born in Musterhausen. He was once attacked by a monster.',\n",
    "        backstory_sonstiges=['id_sci_1']\n",
    "    )\n",
    "    adv.sci.add()\n",
    "    print(enc_obj(adv.sci, id='id_spi_1'))\n",
    "    # print(pre_encode_object(adv.mot, id='id_mot_1'))\n",
    "    print(enc_obj(adv.eus, id='id_eus_1'))\n",
    "    print(enc_obj(adv.npc, id='id_npc_1'))\n",
    "    print(enc_obj(adv.geh, id='id_geh_1'))\n",
    "    print(enc_obj(adv.gru, id='id_gru_1'))\n",
    "    print(enc_obj(adv.bea, id='id_bea_1'))\n",
    "    print(enc_obj(adv.geg, id='id_geg_1'))\n",
    "    print('Spielercharaktere:')\n",
    "    print(enc_obj(adv.sci, id='id_spi_1'))\n",
    "    print(enc_obj(adv.sci, id='id_spi_2'))\n",
    "    print(enc_obj(adv.sci, id='id_spi_3'))\n",
    "    print(enc_obj(adv.sci, id='id_spi_4'))\n",
    "\n",
    "\n",
    "# This function (currently) first cally generate_adventure_objs() to then get the pre-encoding for each object.\n",
    "# It saves the resulting array and prints its overall length. It also writes the variable id_to_pre\n",
    "def main():\n",
    "    id_to_pre = {}\n",
    "    adv = generate_adventure_objs()\n",
    "    adv.save(path='all_objects_adv.json')\n",
    "    name_to_feat = {'sci': adv.sci, 'mot': adv.mot, 'eus': adv.eus, 'npc': adv.npc, 'geh': adv.geh, 'gru': adv.gru,\n",
    "                    'bea': adv.bea, 'geg': adv.geg}\n",
    "    i = 0\n",
    "    all = []\n",
    "    print('start encoding')\n",
    "    for name, cla in name_to_feat.items():\n",
    "        print(name)\n",
    "        for j in range(cla.id_counter):\n",
    "            i += 1\n",
    "            x = enc_obj(cla, id=f'id_{name}_{j}')\n",
    "            all.append(x)\n",
    "            id_to_pre.update({f'id_{name}_{j}': x})\n",
    "    arr = np.array(all)\n",
    "    np.savetxt('allObjectsTwitterEncoded.csv', arr, delimiter=',')\n",
    "    np.save(\"allObjectsTwitterEncoded.npy\", arr)\n",
    "    np.save('id_to_pre.npy', id_to_pre)\n",
    "    print(i)\n",
    "    # Generate A LOT of adventures and their objects.\n",
    "    # train RNN with train_model from RNN\n",
    "    # save the resulting models\n",
    "    # write function RNN to use these saved model\n",
    "    # test enc_obj with optional parameter id_to_pre\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When run, this cell trains an RNN on AllObjectsTwitterEncoded.npy and prints its accuracy. It also provides the function train_model which returns also trains an RNN and returns the encoder decoder pair to use for inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as random\n",
    "\n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model\n",
    "# from __future__ import unicode_literals, print_function, division\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import time\n",
    "import math\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_LENGTH = 10  # change this only when seq2seq.py is not used anymore\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # data is a list of sequences\n",
    "        # A sequence is a list of elements\n",
    "        # element is a list of numbers that encode the elements content.\n",
    "        # the model gets trained to autoencode sequences. It uses an RNN with one cell-run per element.\n",
    "        # The RNN encoding cell takes an element in each cell-run.\n",
    "        # the elements need to be normalized and prepared beforehand.\n",
    "        self.length = len(data)\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.data[idx])\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        # self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is a batch of sequences\n",
    "        # sequence is a list of elements which each consists of multiple numbers.\n",
    "        # embedded = self.dropout(input)  # this does weird stuff\n",
    "        output, hidden = self.gru(input)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # input size equals output size\n",
    "        self.gru = nn.GRU(output_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.element_size = output_size\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        if target_tensor is not None:\n",
    "            batch_size = encoder_outputs.size(0)\n",
    "            decoder_input = torch.empty(batch_size, 1, self.element_size, dtype=torch.float,\n",
    "                                        device=device)  # TODO: find a good start token\n",
    "            decoder_input = torch.zeros(batch_size, 1, self.element_size, dtype=torch.float, device=device)\n",
    "        else:\n",
    "            decoder_input = torch.zeros(1, self.element_size, dtype=torch.float, device=device)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                # decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "                decoder_input = torch.FloatTensor(decoder_output).detach()\n",
    "        if target_tensor is not None:\n",
    "            decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        else:\n",
    "            decoder_outputs = torch.cat(decoder_outputs, dim=0)\n",
    "        # decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None  # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "                decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        # to_l1 = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
    "        # to_l2 = target_tensor.view(-1)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train(train_dataloader, encoder, decoder, n_epochs, lr=0.001,\n",
    "          print_every=100, plot_every=100, encoder_optimizer=None, decoder_optimizer=None, criterion=None):\n",
    "    if criterion is None:\n",
    "        criterion = nn.MSELoss()\n",
    "    if encoder_optimizer is None:\n",
    "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    if decoder_optimizer is None:\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.savefig(\"enc_adv_graph.png\")\n",
    "    print('rnn.py finished and saved .png file.')\n",
    "\n",
    "\n",
    "def to_sequence(data):\n",
    "    # reuse some elements to form new sequences to generate more data. Then there is enough data.\n",
    "    seq = []\n",
    "    clear_data = []\n",
    "    element_length = len(data[0])\n",
    "    for i in range(len(data)):\n",
    "        seq.append(data[i])\n",
    "        if len(seq) == 10 or random.random() > 0.9:\n",
    "            while len(seq) < 10:\n",
    "                seq.append([0. for _ in range(element_length)])\n",
    "            clear_data.append(seq)\n",
    "            seq = []\n",
    "    return clear_data\n",
    "\n",
    "\n",
    "def train_model(data, hidden_size=128, batch_size=32, n_epochs=30, print_every=5, plot_every=5, lr=0.001, encoder_optimizer=None, decoder_optimizer=None, criterion=None):\n",
    "    # this top part is not tested.\n",
    "    # what about .csv files?\n",
    "    if isinstance(data, str):\n",
    "        if data.endswith('.npy'):\n",
    "            data = np.load(data)\n",
    "            data = data.tolist()\n",
    "        elif data.endswith('.pickle'):\n",
    "            import pickle\n",
    "            with open(data, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        elif data.endswith('.json'):\n",
    "            import json\n",
    "            with open(data, 'r') as f:\n",
    "                data = json.load(f)\n",
    "    if not isinstance(data[0][0], list):\n",
    "        data = to_sequence(data)\n",
    "    else:\n",
    "        pass  # check for lengths and if necessary add zeros until all sequences are length = MAX_LENGTH\n",
    "    # data is a list of sequences\n",
    "    # A sequence is a list of elements\n",
    "    # An element is a list of numbers that encode the elements content.\n",
    "    # the model gets trained to auto encode sequences. It uses an RNN with one cell-run per element.\n",
    "    # The RNN encoding cell takes an element in each cell-run.\n",
    "    # the elements need to be normalized and prepared beforehand.\n",
    "    element_length = len(data[0][0])\n",
    "    data = CustomDataset(data)\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    encoder = EncoderRNN(element_length, hidden_size).to(device)\n",
    "    decoder = DecoderRNN(hidden_size, element_length).to(device)\n",
    "\n",
    "    train(dataloader, encoder, decoder, n_epochs=n_epochs, print_every=print_every, plot_every=plot_every, lr=lr, encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer, criterion=criterion)\n",
    "\n",
    "    return encoder, decoder\n",
    "\n",
    "@torch.no_grad()\n",
    "def check(data, encoder, decoder, sequence, l):\n",
    "    # import sklearn as sklearn\n",
    "    out, hid = encoder(torch.FloatTensor(sequence))\n",
    "    result, _, _ = decoder(out, hid)\n",
    "    real_l = l(result, torch.FloatTensor(sequence)).item()\n",
    "    while sequence in data:\n",
    "        data.remove(sequence)\n",
    "    for i in data:\n",
    "        if l(result, torch.FloatTensor(i)).item() < real_l:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def scramble_data(data, n=3):\n",
    "    seqs = []\n",
    "    for i in range(n):\n",
    "        seqs = seqs + to_sequence(data)\n",
    "        random.shuffle(data)\n",
    "    return seqs, to_sequence(data)\n",
    "\n",
    "\n",
    "def acc(data='allObjectsTwitterEncoded.npy', n_epochs=2, hidden_size=128, lr=0.001, criterion=nn.MSELoss(), n=100):\n",
    "    data = np.load(data).tolist()\n",
    "    data, val_data = scramble_data(data)\n",
    "    encoder, decoder = train_model(\n",
    "        data=data,\n",
    "        n_epochs=n_epochs,\n",
    "        hidden_size=hidden_size,\n",
    "        batch_size=32,\n",
    "        print_every=5,\n",
    "        plot_every=5,\n",
    "        lr=lr,\n",
    "        criterion=criterion\n",
    "    )\n",
    "    l = criterion\n",
    "    good = 0\n",
    "    data = random.sample(data, n)\n",
    "    for sequence in data:\n",
    "        if check(data, encoder, decoder, sequence, l):\n",
    "            good += 1\n",
    "    val_good = 0\n",
    "    for sequence in val_data:\n",
    "        if check(val_data, encoder, decoder, sequence, l):\n",
    "            val_good += 1\n",
    "    return good / len(data), val_good / len(val_data)\n",
    "    # choose or generate random sequence\n",
    "    # encode-decode it\n",
    "    # compare result with all sequences\n",
    "    # see how often right one is closest => calculate accuracy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(acc())\n",
    "    # TODO use BCE to reduce error!\n",
    "    # train_model(data='allObjectsTwitterEncodedNumpy.npy', n_epochs=30)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After train_model is defined and id_to_pre.npy is saved, this cell generates the actual encodings. Their quality is based on the accuracy the RNN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "params = {'data': npy_data_path, 'n_epochs': 30, 'hidden_size': 128, 'lr': 0.004, 'criterion': nn.MSELoss()}\n",
    "encoder, decoder = train_model(**params)\n",
    "print(acc(**params))\n",
    "\n",
    "\n",
    "def handle_id_lists(list_of_ids, id_to_pre):\n",
    "    inp = [id_to_pre[i] for i in list_of_ids]\n",
    "    element_length = len(id_to_pre[list_of_ids[0]])\n",
    "    while len(inp) < 10:\n",
    "        inp.append([0. for _ in range(element_length)])\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    output, hidden = encoder(inp)\n",
    "    return list(hidden)\n",
    "    pass  # this function is supposed to return the output of the RNN encoder when fed by the pre_encoding of the\n",
    "    # objects of list_of_ids\n",
    "    return list(range(PRE_RNN_HIDDEN))  # this has the same length\n",
    "\n",
    "def main():\n",
    "    id_to_real = {}\n",
    "    id_to_pre = np.load(\"id_to_pre.npy\", allow_pickle=True)\n",
    "    adv = generate_adventure_objs()\n",
    "    # adv.save(path='all_objects_adv.json')\n",
    "    name_to_feat = {'sci': adv.sci, 'mot': adv.mot, 'eus': adv.eus, 'npc': adv.npc, 'geh': adv.geh, 'gru': adv.gru,\n",
    "                    'bea': adv.bea, 'geg': adv.geg}\n",
    "    i = 0\n",
    "    all = []\n",
    "    print('start encoding')\n",
    "    for name, cla in name_to_feat.items():\n",
    "        print(name)\n",
    "        for j in range(cla.id_counter):\n",
    "            i += 1\n",
    "            x = enc_obj(cla, id=f'id_{name}_{j}', id_to_pre=id_to_pre, handle_id_lists=handle_id_lists)\n",
    "            all.append(x)\n",
    "            id_to_real.update({f'id_{name}_{j}': x})\n",
    "    arr = np.array(all)\n",
    "    np.savetxt('AllObjectsFullEnccoded.csv', arr, delimiter=',')\n",
    "    np.save(\"AllObjectsFullEnccoded.npy\", arr)\n",
    "    np.save('id_to_real.npy', id_to_real)\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
