{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as random\n",
    "\n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model\n",
    "# from __future__ import unicode_literals, print_function, division\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import time\n",
    "import math\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_LENGTH = 10  # change this only when seq2seq.py is not used anymore\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # data is a list of sequences\n",
    "        # A sequence is a list of elements\n",
    "        # element is a list of numbers that encode the elements content.\n",
    "        # the model gets trained to autoencode sequences. It uses an RNN with one cell-run per element.\n",
    "        # The RNN encoding cell takes an element in each cell-run.\n",
    "        # the elements need to be normalized and prepared beforehand.\n",
    "        self.length = len(data)\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.data[idx])\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        # self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is a batch of sequences\n",
    "        # sequence is a list of elements which each consists of multiple numbers.\n",
    "        # embedded = self.dropout(input)  # this does weird stuff\n",
    "        output, hidden = self.gru(input)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # input size equals output size\n",
    "        self.gru = nn.GRU(output_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.element_size = output_size\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        if target_tensor is not None:\n",
    "            batch_size = encoder_outputs.size(0)\n",
    "            decoder_input = torch.empty(batch_size, 1, self.element_size, dtype=torch.float,\n",
    "                                        device=device)  # TODO: find a good start token\n",
    "            decoder_input = torch.zeros(batch_size, 1, self.element_size, dtype=torch.float, device=device)\n",
    "        else:\n",
    "            decoder_input = torch.zeros(1, self.element_size, dtype=torch.float, device=device)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                # decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "                decoder_input = torch.FloatTensor(decoder_output).detach()\n",
    "        if target_tensor is not None:\n",
    "            decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        else:\n",
    "            decoder_outputs = torch.cat(decoder_outputs, dim=0)\n",
    "        # decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None  # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "                decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        # to_l1 = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
    "        # to_l2 = target_tensor.view(-1)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train(train_dataloader, encoder, decoder, n_epochs, lr=0.001,\n",
    "          print_every=100, plot_every=100, encoder_optimizer=None, decoder_optimizer=None, criterion=None):\n",
    "    if criterion is None:\n",
    "        criterion = nn.MSELoss()\n",
    "    if encoder_optimizer is None:\n",
    "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    if decoder_optimizer is None:\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.savefig(\"enc_adv_graph.png\")\n",
    "    print('rnn.py finished and saved .png file.')\n",
    "\n",
    "\n",
    "def to_sequence(data):\n",
    "    # reuse some elements to form new sequences to generate more data. Then there is enough data.\n",
    "    seq = []\n",
    "    clear_data = []\n",
    "    element_length = len(data[0])\n",
    "    for i in range(len(data)):\n",
    "        seq.append(data[i])\n",
    "        if len(seq) == 10 or random.random() > 0.9:\n",
    "            while len(seq) < 10:\n",
    "                seq.append([0. for _ in range(element_length)])\n",
    "            clear_data.append(seq)\n",
    "            seq = []\n",
    "    return clear_data\n",
    "\n",
    "\n",
    "def train_model(data, hidden_size=128, batch_size=32, n_epochs=30, print_every=5, plot_every=5, lr=0.001, encoder_optimizer=None, decoder_optimizer=None, criterion=None):\n",
    "    # this top part is not tested.\n",
    "    # what about .csv files?\n",
    "    if isinstance(data, str):\n",
    "        if data.endswith('.npy'):\n",
    "            data = np.load('allObjectsTwitterEncodedNumpy.npy')\n",
    "            data = data.tolist()\n",
    "        elif data.endswith('.pickle'):\n",
    "            import pickle\n",
    "            with open(data, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        elif data.endswith('.json'):\n",
    "            import json\n",
    "            with open(data, 'r') as f:\n",
    "                data = json.load(f)\n",
    "    if not isinstance(data[0][0], list):\n",
    "        data = to_sequence(data)\n",
    "    else:\n",
    "        pass  # check for lengths and if necessary add zeros until all sequences are length = MAX_LENGTH\n",
    "    # data is a list of sequences\n",
    "    # A sequence is a list of elements\n",
    "    # An element is a list of numbers that encode the elements content.\n",
    "    # the model gets trained to auto encode sequences. It uses an RNN with one cell-run per element.\n",
    "    # The RNN encoding cell takes an element in each cell-run.\n",
    "    # the elements need to be normalized and prepared beforehand.\n",
    "    element_length = len(data[0][0])\n",
    "    data = CustomDataset(data)\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    encoder = EncoderRNN(element_length, hidden_size).to(device)\n",
    "    decoder = DecoderRNN(hidden_size, element_length).to(device)\n",
    "\n",
    "    train(dataloader, encoder, decoder, n_epochs=n_epochs, print_every=print_every, plot_every=plot_every, lr=lr, encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer, criterion=criterion)\n",
    "\n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "def check(data, encoder, decoder, sequence, l):\n",
    "    # import sklearn as sklearn\n",
    "    out, hid = encoder(torch.FloatTensor(sequence))\n",
    "    result, _, _ = decoder(out, hid)\n",
    "    real_l = l(result, torch.FloatTensor(sequence)).item()\n",
    "    while sequence in data:\n",
    "        data.remove(sequence)\n",
    "    for i in data:\n",
    "        if l(result, torch.FloatTensor(i)).item() < real_l:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def scramble_data(data, n=3):\n",
    "    seqs = []\n",
    "    for i in range(n):\n",
    "        seqs = seqs + to_sequence(data)\n",
    "        random.shuffle(data)\n",
    "    return seqs, to_sequence(data)\n",
    "\n",
    "\n",
    "def acc(data='allObjectsTwitterEncodedNumpy.npy'):\n",
    "    data = np.load(data).tolist()\n",
    "    data, val_data = scramble_data(data)\n",
    "    encoder, decoder = train_model(data=data, n_epochs=60)\n",
    "    l = torch.nn.MSELoss()\n",
    "    good = 0\n",
    "    for sequence in data:\n",
    "        if check(data, encoder, decoder, sequence, l):\n",
    "            good += 1\n",
    "    val_good = 0\n",
    "    for sequence in val_data:\n",
    "        if check(val_data, encoder, decoder, sequence, l):\n",
    "            good += 1\n",
    "    return good / len(data), val_good / len(val_data)\n",
    "    # choose or generate random sequence\n",
    "    # encode-decode it\n",
    "    # compare result with all sequences\n",
    "    # see how often right one is closest => calculate accuracy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(acc())\n",
    "    # TODO use BCE to reduce error!\n",
    "    # train_model(data='allObjectsTwitterEncodedNumpy.npy', n_epochs=30)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
